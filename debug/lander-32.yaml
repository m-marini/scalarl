---
version: "5"
env:
  dt: 0.25
  h0Range: 100
  z0: 100
  fuel: 300
  zMax: 150
  hRange: 500
  zRange: 150
  landingRadius: 10
  landingVH: 2
  landingVZ: 4
  g: 1.6
  maxAH: 1
  maxAZ: 3.2
  #landedReward: 100
  #vCrashReward: -100
  #hCrashReward: -10
  #outOfPlatformReward: -3
  #outOfRangeReward: -100
  #outOfFuelReward: -100
  #flyingReward: 0
  #vSpeedReward: -2 
  #hSpeedReward: -2
  landedReward: 100
  vCrashReward: -100
  hCrashReward: -10
  outOfPlatformReward: -3
  outOfRangeReward: -100
  outOfFuelReward: -100
  flyingReward: 0
  directionReward: 1
  hSpeedReward: 0
  vSpeedReward: -2
  # input encoder
  encoder: LanderTiles
  signalRanges:
    - [-1, 1]
    - [-1, 1]
    - [0, 64]
    - [0, 16]
    - [0, 16]
    - [-16, 16]
  # output encoder
  actionConfig: DiscreteActions
agent:
  type: ActorCritic
  avgReward: -6
  rewardDecay: 0.999
  valueDecay: 0.99
  rewardRange: [-100, 100]
  network:
    seed: 1234
    numHiddens:
    - 100
    - 30
    - 30
    shortcuts:
      - [ 1, 4 ]
    activation: SOFTPLUS
    updater: Sgd
    learningRate: 10e-3
    maxAbsGradients: 100
    maxAbsParameters: 10e3
    dropOut: 0.8
  actors:
    - type: PolicyActor
      alpha: 30e-3
      prefRange: [-2.3, 2.3]
    - type: PolicyActor
      alpha: 10e-3
      prefRange: [-2.3, 2.3]
    - type: PolicyActor
      alpha: 20e-3
      prefRange: [-2.3, 2.3]
  planner:
    planningSteps: 5
    threshold: 10e-6
    minModelSize: 300
    maxModelSize: 1000
    stateKey:
      type: Discrete
    actionsKey:
      type: Discrete
session:
  numSteps: 3000
  seed: 1234
