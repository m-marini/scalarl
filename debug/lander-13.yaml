---
version: "5"
env:
  dt: 0.25
  g: 1.6
  fuel: 300
  initialLocationRange:
    - [-100, 100]
    - [-100, 100]
    - [100, 100]
  spaceRanges:
    - [-500, 500]
    - [-500, 500]
    - [0, 150]
  landingRadius: 10
  landingSpeedLimits: [2, -4]
  optimalSpeed: [1, -2]
  jetAccRange:
    - [-1, 1]
    - [-1, 1]
    - [0, 3.2]
  landedReward: 100
  vCrashedOnPlatformReward: 100
  hCrashedOnPlatformReward: 100
  landedOutOfPlatformReward: -30
  vCrashedOutOfPlatformReward: -30
  hCrashedOutOfPlatformReward: -30
  outOfRangeReward: -100
  outOfFuelReward: -100
  flyingReward: -6
  directionReward: 5
  hSpeedReward: -5
  vSpeedReward: -1.25
  actionDimensions: 3
  encoder: LanderContinuous
  signalRanges:
    - [-3.14, 3.14]
    - [-3.14, 3.14]
    - [0, 64]
    - [0, 16]
    - [0, 8]
    - [-16, 16]
agent:
  type: ActorCritic
  avgReward: 0
  rewardDecay: 0.999
  valueDecay: 0.99
  rewardRange: [-100, 100]
  network:
    seed: 1234
    activation: SOFTPLUS
    numHiddens:
    - 100
    - 30
    - 30
    shortcuts:
      - [ 1, 4 ]
    updater: Sgd
    learningRate: 1000e-3
    maxAbsGradients: 100
    maxAbsParameters: 10e3
    dropOut: 0.8
  planner:
    planningSteps: 5
    threshold: 1e-3
    minModelSize: 300
    maxModelSize: 1000
    stateKey:
      type: NormalTiles
      noTiles: [32, 32, 32, 32, 32, 32]
    actionsKey:
      type: Discrete
  actors:
    - type: PolicyActor
      noValues: 8
      range: 
        - [0, 6.09]
      alpha: 100e-3
      prefRange:
        - [-2.4, 2.4]
    - type: PolicyActor
      noValues: 3
      range:
        - [0, 3]
      alpha: 100e-3
      prefRange:
        - [-2.4, 2.4]
    - type: PolicyActor
      noValues: 5
      range:
        - [-6, 6]
      alpha: 100e-3
      prefRange:
        - [-2.4, 2.4]
session:
  numSteps: 1000000
  seed: 1234
