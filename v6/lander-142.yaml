---
version: "6"
env:
  dt: 0.25
  g: 1.6
  fuel: 244
  initialLocationRange:
    - [-100, 100]
    - [-100, 100]
    - [80, 100]
  spaceRanges:
    - [-500, 500]
    - [-500, 500]
    - [0, 150]
  landingRadius: 10
  landingSpeedLimits: [2, -4]
  optimalSpeedRanges:
   - [0, 1]
   - [-2, 0]
  jetAccRange:
    - [-1, 1]
    - [-1, 1]
    - [0, 3.2]
  rewards:
    flying:
      base: -0.06
      direction: -284e-6
    landed:
      base: 1
    hCrashedOnPlatform:
      base: -24.4
      hspeed: -0.25
      vspeed: -0.125
    vCrashedOnPlatform:
      base: -24.4
      hspeed: -0.25
      vspeed: -0.125
    landedOutOfPlatform:
      base: -21.3
      distance: -0.4
    hCrashedOutOfPlatform:
      base: -22.4
      distance: -0.4
      hspeed: -0.25
      vspeed: -0.125
    vCrashedOutOfPlatform:
      base: -22.4
      distance: -0.4
      hspeed: -0.25
      vspeed: -0.125
    outOfFuel:
      base: -27.4
      distance: -0.4
      height: -0.2
    outOfRange:
      base: -28.4
      distance: -0.4
  actionDimensions: 3
agent:
  type: ActorCritic
  #modelPath: lander-142
  stateEncoder:
    type: Tiles
    ranges:
    - [-3.14, 3.14]
    - [-3.14, 3.14]
    - [0, 100]
    - [0, 150]
    - [0, 8]
    - [-8, 8]
    noTiles: [3, 3, 3, 3, 3, 3]
  avgReward: -0.1
  rewardDecay: 0.999 # tau = -dt / ln 0.999 = 250 s
  valueDecay: 0.99 # tau = -dt / ln 0.999 = 25 s
  rewardRange:
  - [-30, -4]
  network:
#    seed: 1234
    learningRate: 10e-3
    activation: SOFTPLUS
    numHidden: []
    shortcuts: []
    updater: Sgd
    maxAbsGradient: 100
    maxAbsParameter: 10e3
    dropOut: 0.8
  actors:
    - type: PolicyActor
      noValues: 8
      range: 
        - [0, 6.09]
      alpha: 1000e-3
      epsilon: 0.05
      alphaDecay: 0.999
      prefRange:
        - [-1.15, 1.15]
    - type: PolicyActor
      noValues: 3
      range:
        - [0, 3]
      alpha: 1000e-3
      epsilon: 0.05
      alphaDecay: 0.999
      prefRange:
        - [-1.15, 1.15]
    - type: PolicyActor
      noValues: 5
      range:
        - [-6, 6]
      alpha: 1000e-3
      epsilon: 0.05
      alphaDecay: 0.999
      prefRange:
        - [-1.15, 1.15]
  planner:
    planningSteps: 2
    threshold: 1e-3
    minModelSize: 300
    maxModelSize: 1000
    stateKey:
      type: Discrete
      noValues: [32, 32, 32, 32, 32, 32]
      ranges:
      - [-3.14, 3.14]
      - [-3.14, 3.14]
      - [0, 64]
      - [0, 16]
      - [0, 8]
      - [-16, 16]
    actionsKey:
      type: Discrete
      noValues: [8, 3, 5]
      ranges:
        - [0, 6.28]
        - [0, 3]
        - [-6, 6]
session:
  numSteps: 30000
#  seed: 1234
  kpisOnPlanning: false
  modelPath: lander-142
