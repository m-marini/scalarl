---
version: "5"
env:
  type: LanderContinuous
  actionConfig: DiscreteActions
  dt: 0.25
  h0Range: 10
  z0: 20
  fuel: 100
  zMax: 150
  hRange: 500
  zRange: 150
  vhRange: 24
  vzRange: 12
  landingRadius: 10
  landingVH: 2
  landingVZ: 4
  g: 1.6
  maxAH: 1
  maxAZ: 3.2
  landedReward: 1
  vCrashReward: -1
  hCrashReward: -100e-3
  outOfPlatformReward: -30e-3
  outOfRangeReward: -1
  outOfFuelReward: -1
  rewardDistanceScale: 0
  flyingReward: -10e-3
  normalize:
    clipMin: [-10, -10, -10, -24, -24, -12]
    clipMax: [10, 10, 10, 24, 24, 12]
    offset: [0, 0, 0, 0, 0, 0]
    max: [20, 20, 20, 8, 8, 4]
agent:
  type: ActorCritic
  avgReward: 0
  rewardDecay: 0.99
  valueDecay: 0.99
  rewardRange: [-1, 1]
  network:
    numHiddens:
    - 100
    - 30
    - 30
    shortcuts:
      - [ 1, 4 ]
    updater: Sgd
    learningRate: 100e-6
    maxAbsGradients: 100
    maxAbsParameters: 10e3
    dropOut: 0.8
  actors:
    - type: PolicyActor
      alpha: 7
      prefRange: [-7, 7]
    - type: PolicyActor
      alpha: 7
      prefRange: [-7, 7]
    - type: PolicyActor
      alpha: 7
      prefRange: [-7, 7]
  planner:
    planningSteps: 5
    threshold: 10e-6
    minModelSize: 300
    maxModelSize: 1000
    stateKey:
      type: Tiles
      offset: [-1, -1, -1, -1, -1, -1]
      max: [1, 1, 1, 1, 1, 1]
      tiles: [100, 100, 100, 100, 100, 100]
    actionsKey:
      type: Discrete
session:
  numSteps: 3000
