---
version: "4"
env:
  type: LanderContinuous
  actionConfig: DiscreteActions
  dt: 0.25
  h0Range: 10
  z0: 3
  fuel: 30
  zMax: 150
  hRange: 500
  zRange: 150
  vhRange: 24
  vzRange: 12
  landingRadius: 10
  landingVH: 0.5
  landingVZ: 4
  g: 1.6
  maxAH: 1
  maxAZ: 3.2
  landedReward: 10
  crashReward: -10
  outOfRangeReward: -10
  outOfFuelReward: -10
  rewardDistanceScale: 0
  flyingReward: -1
  normalize:
    clipMin: [-500, -500, -150, -24, -24, -12]
    clipMax: [500, 500, 150, 24, 24, 12]
    offset: [0, 0, 0, 0, 0, 0]
    max: [10, 10, 4, 0.4, 0.4, 0.5]
agent:
  type: ActorCritic
  avgReward: 0
  rewardDecay: 0.99
  valueDecay: 0.99
  network:
    numHiddens:
    - 100
    - 100
    shortcuts:
      - [ 0, 3 ]
    updater: Sgd
    autoScaleLearningRate: 10
    maxAbsGradients: 1
    maxAbsParameters: 10e3
    dropOut: 0.8
  actors:
    - type: PolicyActor
      alpha: 1
    - type: PolicyActor
      alpha: 1
    - type: PolicyActor
      alpha: 1
session:
  numSteps: 3000
  dump: v4/dump-113.csv
  kpiFile: debug/kpi-113.csv
